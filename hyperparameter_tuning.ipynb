{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fdf878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a819e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train/Validation/Test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd799bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, lr, num_layers, neurons, dropout, weight_decay):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        model.add(layers.Dense(\n",
    "            neurons,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(weight_decay)\n",
    "        ))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_config():\n",
    "    return {\n",
    "        \"lr\": 10 ** random.uniform(-5, -2),\n",
    "        \"batch_size\": ([16]),\n",
    "        \"num_layers\": random.randint(6, 10),\n",
    "        \"neurons\": random.choice([32, 64, 128, 256]),\n",
    "        \"dropout\": random.uniform(0.0, 0.5),\n",
    "        \"weight_decay\": 10 ** random.uniform(-6, -2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4f1d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1 - Training for 5 epochs\n",
      "\n",
      "Stage 2 - Training for 15 epochs\n",
      "\n",
      "Stage 3 - Training for 30 epochs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, cfg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(configs):\n\u001b[32m     13\u001b[39m     model = build_model(\n\u001b[32m     14\u001b[39m         X_train.shape[\u001b[32m1\u001b[39m],\n\u001b[32m     15\u001b[39m         cfg[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m         cfg[\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     20\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     val_loss = history.history[\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n\u001b[32m     31\u001b[39m     stage_results.append((cfg, val_loss, history))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Quantam Dev\\Desktop\\Sem 8\\dl\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Quantam Dev\\Desktop\\Sem 8\\dl\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:398\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbegin_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m         logs = \u001b[38;5;28mself\u001b[39m.train_function(iterator)\n\u001b[32m    400\u001b[39m         callbacks.on_train_batch_end(end_step, logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Quantam Dev\\Desktop\\Sem 8\\dl\\.venv\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:173\u001b[39m, in \u001b[36mCallbackList.on_train_batch_begin\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m    171\u001b[39m         callback.on_epoch_end(epoch, logs)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_batch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    174\u001b[39m     logs = python_utils.pythonify_logs(logs)\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "configs = [sample_config() for _ in range(64)]\n",
    "history_store = {}\n",
    "results = []\n",
    "\n",
    "epochs_schedule = [5, 15, 30]\n",
    "\n",
    "for stage, epochs in enumerate(epochs_schedule):\n",
    "    print(f\"\\nStage {stage+1} - Training for {epochs} epochs\")\n",
    "\n",
    "    stage_results = []\n",
    "\n",
    "    for i, cfg in enumerate(configs):\n",
    "        model = build_model(\n",
    "            X_train.shape[1],\n",
    "            cfg[\"lr\"],\n",
    "            cfg[\"num_layers\"],\n",
    "            cfg[\"neurons\"],\n",
    "            cfg[\"dropout\"],\n",
    "            cfg[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=cfg[\"batch_size\"],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        val_loss = history.history[\"val_loss\"][-1]\n",
    "        stage_results.append((cfg, val_loss, history))\n",
    "        history_store[str(cfg)] = history\n",
    "\n",
    "    stage_results.sort(key=lambda x: x[1])\n",
    "    configs = [cfg for cfg, _, _ in stage_results[:len(stage_results)//2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc11193",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_8 = stage_results[:8]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for i, (cfg, _, hist) in enumerate(best_8):\n",
    "    plt.plot(hist.history['loss'], label=f'Config {i+1}')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Loss vs Epochs for Best 8 Configurations\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(cfg):\n",
    "    model = build_model(\n",
    "        X_train.shape[1],\n",
    "        cfg[\"lr\"],\n",
    "        cfg[\"num_layers\"],\n",
    "        cfg[\"neurons\"],\n",
    "        cfg[\"dropout\"],\n",
    "        cfg[\"weight_decay\"]\n",
    "    )\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=cfg[\"batch_size\"], verbose=0)\n",
    "\n",
    "    preds = model.predict(X_test).flatten()\n",
    "    return {\n",
    "        \"R2\": r2_score(y_test, preds),\n",
    "        \"MAE\": mean_absolute_error(y_test, preds),\n",
    "        \"MSE\": mean_squared_error(y_test, preds)\n",
    "    }\n",
    "\n",
    "metrics_table = []\n",
    "for i, (cfg, _, _) in enumerate(best_8):\n",
    "    metrics = evaluate_model(cfg)\n",
    "    metrics_table.append([i+1, \"MSE\", metrics[\"R2\"], metrics[\"MAE\"], metrics[\"MSE\"]])\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    metrics_table,\n",
    "    columns=[\"Configuration #\", \"Loss Function\", \"RÂ² Score\", \"MAE\", \"MSE\"]\n",
    ")\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_4 = best_8[:4]\n",
    "\n",
    "hp_table = []\n",
    "for i, (cfg, _, _) in enumerate(best_4):\n",
    "    hp_table.append([\n",
    "        i+1,\n",
    "        cfg[\"lr\"],\n",
    "        cfg[\"batch_size\"],\n",
    "        cfg[\"num_layers\"],\n",
    "        cfg[\"neurons\"],\n",
    "        cfg[\"dropout\"],\n",
    "        cfg[\"weight_decay\"]\n",
    "    ])\n",
    "\n",
    "hp_df = pd.DataFrame(\n",
    "    hp_table,\n",
    "    columns=[\n",
    "        \"Configuration #\",\n",
    "        \"Learning rate\",\n",
    "        \"Batch size\",\n",
    "        \"Hidden layers\",\n",
    "        \"Neurons per layer\",\n",
    "        \"Dropout rate\",\n",
    "        \"Weight decay\"\n",
    "    ]\n",
    ")\n",
    "hp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96f321",
   "metadata": {},
   "source": [
    "Hyperparameter tuning significantly improves model performance. Random search combined with successive halving efficiently explored the search space while reducing computational cost.\n",
    "\n",
    "Lower learning rates, deeper networks, and moderate regularization consistently produced better generalization performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
